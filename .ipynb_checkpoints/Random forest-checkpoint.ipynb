{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Ensemble Techniques "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        ###############    #############   Ensemble techniques and it's types   ###############   ###############\n",
    " \n",
    " =>   Ensemble technique has a similar underlying idea where we aggregate predictions from a group of predictors, \n",
    "       which may be classifiers or regressors, and most of the times the prediction is better than the one obtained using a\n",
    "         single predictor. Such algorithms are called Ensemble methods and such predictors are called Ensembles.\n",
    "\n",
    " =>  Ensemble Techniques are such type of techniques, where we try to implement multiple Decision Making algorithms together\n",
    "        and then it gives a filanalized decision by taking Average or Voting of outcomes.\n",
    "    \n",
    " =>  Ensemble methods take multiple small models and combine their predictions to obtain a more powerful predictive power.\n",
    "\n",
    "        \n",
    " =>  Inside Ensemble techniques, We get 3 different techniques.\n",
    "\n",
    "       *  Bagging or Bootstrap Aggregation\n",
    "       *  Boosting  &\n",
    "       *  Stacking\n",
    "    \n",
    " =>  Ensemble techniques are used in classification problems as well as Regression problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/ensemble.PNG\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Bagging  or  Bootstrap Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "          #############   ################   Bagging  or  Bootstrap Aggregation   #################   ##############\n",
    "    \n",
    " =>  Bootstrapping is a technique of sampling different sets of data from a given training set by using overlapping \n",
    "      ( or replacement). After bootstrapping the training dataset, we train model on all the different sets and aggregate \n",
    "        the result. This technique is known as Bootstrap Aggregation or Bagging.\n",
    "            \n",
    " =>  Bagging is the type of Ensemble technique in which a single training algorithm is used on different subsets of \n",
    "       the training data where the subset sampling is done with replacement (bootstrap).\n",
    "     Once the algorithm is trained on all the subsets, then Bagging makes the prediction by aggregating all the predictions\n",
    "        made by the algorithm on different subsets. \n",
    "        \n",
    " =>  In case of Regression, Bagging prediction is simply the mean of all the predictions and in the case of Classification,\n",
    "       Bagging prediction is the most frequent prediction (majority vote) among all the predictions.\n",
    "\n",
    " =>  Bagging is also known as Parallel model since we run all models parallely and combine there results at the end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/1.PNG\">                           \n",
    "<img src=\"images/3.PNG\"> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "      ################    ##############   Out of bag  Evaluation   ################    ################\n",
    "\n",
    " =>  In Bagging, when different samples are collected, no sample contains all the data but a fraction of the original dataset.\n",
    "      and there might be some data which are never sampled at all.\n",
    "        \n",
    " =>  The remaining data which are not sampled are called Out of Bag instances. Since the model never trains over these data, \n",
    "        they can be used for evaluating the accuracy of the model by using these data for predicition.\n",
    "       \n",
    " =>  We do not need validation set or cross validation and can use out of bag instances for that purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "       #################   ###############   Bagging Classifiers and Regressors   ################   ###############     \n",
    "    \n",
    " =>  A Bagging classifier is an ensemble meta-estimator that fits base classifiers each on random subsets of the original \n",
    "        dataset and then aggregate their individual predictions by voting to form a final prediction.\n",
    "                \n",
    " =>  A Bagging regressor is an ensemble meta-estimator that fits base Regressors each on random subsets of the original dataset\n",
    "       and then aggregate their individual predictions by averaging to form a final prediction. \n",
    "            \n",
    " =>  When random subsets of the dataset are drawn as random subsets of the samples, then this algorithm is known as Pasting.\n",
    " \n",
    " =>  If samples are drawn with replacement, then the method is known as Bagging.\n",
    "\n",
    " =>  When random subsets of the dataset are drawn as random subsets of the features, then the method is known as \n",
    "       Random Subspaces.\n",
    "        \n",
    " =>  When base estimators are built on subsets of both samples and features, then the method is known as Random Patches.\n",
    "    \n",
    "    \n",
    " =>  Bagging Classifier & Regressor class should be imported from sklearn.\n",
    "\n",
    "        >>> from sklearn.ensemble import BaggingClassifier, BaggingRegressor\n",
    "        \n",
    " =>  After importing, we make an object of imported Bagging function and then uses a Base ML Classifier algorithm inside it.\n",
    "\n",
    "        >>> bag_model = BaggingClassifier(base_estimator, n_estimator, max_sample, max_features, bootstrap, bootstrap=-features,\n",
    "                                         oob_score, warm_start, n_jobs, random_state, verbose)  \n",
    "    \n",
    " =>  Bagging Classifier and Bagging Regressor both have similar parameters.\n",
    "    \n",
    " \n",
    "    ###################   ###############   Parameters of a Bagging Classifier object   ################   ##################\n",
    "    \n",
    "         Parameters\n",
    "         ----------\n",
    " * base_estimator : object, default=None\n",
    "       \n",
    "        The base estimator to fit on random subsets of the dataset.\n",
    "        If None, then the base estimator is a decision tree.\n",
    "\n",
    " * n_estimators : int, default=10   ->  The number of base estimators in the ensemble.\n",
    "\n",
    " * max_samples : int or float, default=1.0\n",
    "    \n",
    "       The number of samples to draw from X to train each base estimator (with replacement by default) .\n",
    "                                                                          \n",
    "        - If int, then draw `max_samples` samples.\n",
    "        - If float, then draw `max_samples * X.shape[0]` samples.\n",
    "\n",
    " * max_features : int or float, default=1.0\n",
    "   \n",
    "    The number of features to draw from X to train each base estimator (without replacement by default).\n",
    "\n",
    "        - If int, then draw `max_features` features.\n",
    "        - If float, then draw `max_features * X.shape[1]` features.\n",
    "\n",
    " * bootstrap : bool, default=True\n",
    "   \n",
    "      Whether samples are drawn with replacement. If False, sampling without replacement is performed.\n",
    "\n",
    " * bootstrap_features : bool, default=False    ->   Whether features are drawn with replacement.\n",
    "\n",
    " * oob_score : bool, default=False   ->  Whether to use out-of-bag samples to estimate the generalization error.\n",
    "\n",
    " * warm_start : bool, default=False\n",
    "    \n",
    "    When set to True, reuse the solution of the previous call to fit and add more estimators to the ensemble, otherwise, \n",
    "    just fit a whole new ensemble. See :term:`the Glossary <warm_start>`.\n",
    "\n",
    " * n_jobs : int, default=None  ->  The numbe r of jobs to run in parallel for both fit() and predict() methods.\n",
    "                                      None means 1 and -1 means all.\n",
    "    \n",
    " * random_state : int or RandomState, default=None\n",
    "    \n",
    "    Controls the random resampling of the original dataset (sample wise and feature wise).\n",
    "   \n",
    "    If the base estimator accepts a `random_state` attribute, a different seed is generated for each instance in the ensemble.\n",
    "    Pass an int for reproducible output across multiple function calls.\n",
    "   \n",
    " * verbose : int, default=0  ->  Controls the verbosity when fitting and predicting.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "\n",
    " *  base_estimator_ : estimator  ->  The base estimator from which the ensemble is grown.\n",
    "\n",
    " *  n_features_ : int   ->  The number of features when :meth:`fit` is performed.\n",
    "\n",
    " *  estimators_ : list of estimators  ->  The collection of fitted base estimators.\n",
    "\n",
    " *  estimators_samples_ : list of arrays\n",
    "    \n",
    "    The subset of drawn samples (i.e., the in-bag samples) for each base estimator and each subset is defined by an array\n",
    "       of the indices selected.\n",
    "\n",
    " * estimators_features_ : list of arrays  ->  The subset of drawn features for each base estimator.\n",
    "\n",
    " * classes_ : ndarray of shape (n_classes)  ->  The classes labels.\n",
    "\n",
    " * n_classes_ : int or list  ->  The number of classes.\n",
    "\n",
    " * oob_score_ : float\n",
    "    \n",
    "    Score of the training dataset obtained using an out-of-bag estimate.\n",
    "    This attribute exists only when ``oob_score`` is True.\n",
    "\n",
    " * oob_decision_function_ : ndarray of shape (n_samples, n_classes)\n",
    "    \n",
    "    Decision function computed with out-of-bag estimate on the training set. \n",
    "    \n",
    "    If n_estimators is small it might be possible that a data point was never left out during the bootstrap. \n",
    "     In this case, oob_decision_function_ might contain NaN. This attribute exists only when ``oob_score`` is True.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Python implementation of Bagging models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple Knn score ->  0.916083916083916\n",
      "Bagging KNN score ->  0.9370629370629371\n"
     ]
    }
   ],
   "source": [
    "#  loading Breast cancer dataset\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "dataset = load_breast_cancer()\n",
    "\n",
    "#  Splitting Feature and label\n",
    "X = dataset.data\n",
    "y = dataset.target\n",
    "\n",
    "#  Splitting dataset into Train and Test\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=3)\n",
    "\n",
    "#  Importing KNN Classifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "#  Fitting KnnClasifier over data\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train, y_train)\n",
    "print ('Simple Knn score -> ', knn.score(X_test, y_test))\n",
    "\n",
    "#  Importing Bagging classifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "# Going with Bagging knn\n",
    "bag_knn = BaggingClassifier(KNeighborsClassifier(n_neighbors=5), n_estimators=10, max_samples=0.5, bootstrap=True,\n",
    "                            random_state=3,oob_score=True) \n",
    "\n",
    "#  fitting bag_knn on Train data\n",
    "bag_knn.fit(X_train, y_train)\n",
    "print ('Bagging KNN score -> ', bag_knn.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "            #############   ############    Advantages  of  Bagging    ##############   ##############\n",
    "\n",
    " 1)  Bagging significantly decreases the variance without increasing bias. \n",
    "\n",
    " 2)  Bagging methods work so well because of diversity in the training data since the sampling is done by Bootstraping.\n",
    "\n",
    " 3)  If the training set is very huge, it can save computional time by training model on relatively smaller data set and\n",
    "       still can increase the accuracy of the model.\n",
    "\n",
    " 4)  Works well with small datasets as well.\n",
    "\n",
    "        #############   ############   Disadvantages  of  Bagging    ##############   ##############\n",
    "\n",
    " => The main disadvantage of Bagging is that it improves the accuracy of the model on the expense of interpretability.\n",
    "     i.e. if a single tree was being used as the base model, then it would have a more attarctive and easily interpretable\n",
    "       diagram, but with use of bagging this interpretability gets lost.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "            ##############   ##############   Pasting  Technique    ###############    ##############\n",
    "\n",
    " =>  Pasting is an Ensemble technique similar to Bagging with the only difference being that there is no replacement\n",
    "      or overlapping done while sampling the training dataset. \n",
    "    \n",
    " =>  This causes less diversity in the sampled datasets and data ends up being correlated.\n",
    "       Thats why Bagging is more preffered than pasting in real scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging Knn score with Pasting ->  0.9300699300699301\n"
     ]
    }
   ],
   "source": [
    "pasting_knn = BaggingClassifier(KNeighborsClassifier(n_neighbors=5), n_estimators=10, max_samples=0.5, bootstrap=False,\n",
    "                                random_state=3) \n",
    "\n",
    "pasting_knn.fit(X_train, y_train)\n",
    "print ('Bagging Knn score with Pasting -> ', pasting_knn.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "           ################   ###########   Indtroduction to Random forest   #############   ################# \n",
    "    \n",
    " =>  Decision trees are one of such models which have low bias but high variance and we have studied that Decision trees\n",
    "       tends to overfit the data. So Bagging technique becomes a very good solution for decreasing the variance in\n",
    "          a Decision tree.\n",
    "\n",
    " =>  Instead of using a Bagging model with a Base model as a Decision tree, we can also use Random forest which is more\n",
    "       convenient and well optimized for Decision trees.\n",
    "    \n",
    " =>  The main issue with bagging is that there is not much independence among the sampled datasets i.e. there is correlation. \n",
    "     \n",
    " =>  A Random Forest model is made up of a large number of small Decision trees, called Estimators, which each produce their\n",
    "       own predictions and a Random Forest model combines the predictions of the estimators to produce a more accurate\n",
    "         and robust prediction.\n",
    "    \n",
    " =>  The advantage of random forests over Bagging models is that the Random forests makes a tweak in the working algorithm \n",
    "        of Bagging model to decrease the correlation in trees.  \n",
    "      The idea is to introduce more randomness while creating trees which will help in reducing correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "          ############   ############   Working of a Random Forest   ###############    ##############\n",
    "\n",
    " =>  Just like in bagging, different samples are collected from the training dataset using Bootstraping.\n",
    "        and on each sample we train our tree model and we allow the trees to grow with high depths. \n",
    "    \n",
    " =>  Now, the difference with in Random forest is how the trees are formed. \n",
    "    In bootstraping we allow all the sample data to be used for splitting the nodes but not with random forests. \n",
    "     When building a decision tree, each time a split is to happen, a random sample of ‘m’ predictors are chosen \n",
    "        from the total ‘p’ predictors. Only those ‘m’ predictors are allowed to be used for the split.\n",
    "        \n",
    "    Why is that?\n",
    "    \n",
    " =>  Suppose in those ‘p’ predictors, 1 predictor is very strong. Now each sample this predictor will remain the strongest.\n",
    "     So, whenever trees will be built for these sampled data, this predictor will be chosen by all the trees for splitting\n",
    "      and thus will result in similar kind of tree formation for each bootstrap model. \n",
    "        \n",
    "     This introduces correaltion in the dataset and averaging correalted dataset results do not lead low variance.\n",
    "      That’s why in Random Forest the choice for selecting node for split is limited and it introduces randomness in \n",
    "         the formation of the trees as well.\n",
    "    \n",
    "     Most of the predictors are not allowed to be considered for split.\n",
    "        Generally, value of ‘m’ is taken as m ≈√p , where ‘p’ is the number of predictors in the sample.\n",
    "        When m=p , the random forest model becomes bagging model.   \n",
    "     \n",
    "    This method is also referred as “Feature Sampling”\n",
    "    \n",
    " =>  The above graph represents the decrease in test classifcation error as we select different values  of ‘m’.\n",
    "\n",
    " =>  Once the trees are formed, prediction is made by the random forest by aggregating the predictions of all the model. \n",
    "     For regression model, the mean of all the predictions is the final prediction and for classification mode,\n",
    "        the mode of all the predictions is considered the final predictions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/7.PNG\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Working of a Random Forest Model\n",
    "\n",
    "<img src=\"images/random_forest.PNG\">\n",
    "\n",
    "From the given dataset different samples are created by bootstrapping and these samples are used to train different Decision trees.\n",
    "\n",
    "Once the training is complete, prediction is made using all the different models.\n",
    "\n",
    "\n",
    "#### Predicting Outcome\n",
    "\n",
    "<img src=\"images/random_forest2.PNG\">"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    " =>  Random forest makes the prediction by taking the mode of all the predictions made by all the models, \n",
    "       since this is the case of classification. This process is also known as “Majority voting”.\n",
    "\n",
    " =>  We can also use prediction probability to make the final prediction. We can use the predict_proba method, \n",
    "      which will predict a probability from 0 to 1 that a given class is the right one for a row. \n",
    "     For a problem with output being only 0 and 1, we will get a matrix with as many rows as there\n",
    "        is in the data and 2 columns. predict_proba will return something like this.\n",
    "        \n",
    " =>  Each row corresponds to a prediction. \n",
    " \n",
    " =>  The first column is the probability that the prediction is a 0, the second column is the probability that the prediction \n",
    "       is a 1. Each row adds up to 1.\n",
    "\n",
    " =>  If we just take the second column, we get the average value that the classifier would predict for that row. \n",
    "       If there's a .9 probability that the correct classification is 1, we can use the .9 as the value the classifier is            predicting. This will give us a continuous output in a single vector instead of just 0 or 1.\n",
    "\n",
    " =>  We can then add all of the vectors we get through this method together and divide by the number of vectors to get \n",
    "       the mean prediction by all the members of the ensemble. We can then round off to get 0 or 1 predictions.\n",
    "\n",
    " =>  Similarly, in case of regression Random forest makes the prediction by taking the mean of all the predictions made by              different models. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/8.PNG\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "     ################   ############   Advantages and Disadvantages of Random Forest    ##############   ###############\n",
    "\n",
    " =>  It can be used for both regression and classification problems.\n",
    "\n",
    " =>  Since base model is a tree, handling of missing values is easy.\n",
    "\n",
    " =>  It gives very accurate result with very low variance.\n",
    "\n",
    " =>  Results of a random forest are very hard to interpret in comparison with decision trees.\n",
    "\n",
    " =>  High computational time than other respective models.\n",
    "\n",
    "\n",
    "** Note **  ->  Random Forest should be used where accuracy is up utmost priority and interpretability is not very important.\n",
    "                    Also, computational time is less expensive than the desired outcome.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-Validation\n",
    "\n",
    "Suppose you train a model on a given dataset using any specific algorithm. You tried to find the accuracy of the trained model using the same training data and found the accuracy to be 95% or maybe even 100%. What does this mean? Is your model ready for prediction? The answer is no.\n",
    "Why? Because your model has trained itself on the given data, i.e. it knows the data and it has generalized over it very well. But when you try and predict over a new set of data, it’s most likely to give you very bad accuracy, because it has never seen the data before and thus it fails to generalizes well over it. This is the problem of overfitting. \n",
    "To tackle such problem, Cross-validation comes into the picture. Cross-validation is a resampling technique with a basic idea of dividing the training dataset into two parts i.e. train and test. On one part(train) you try to train the model and on the second part(test) i.e. the data which is unseen for the model, you make the prediction and check how well your model works on it. If the model works with good accuracy on your test data, it means that the model has not overfitted the training data and can be trusted with the prediction, whereas if it performs with bad accuracy then our model is not to be trusted and we need to tweak our algorithm.\n",
    "\n",
    "\n",
    "Let’s see the different approaches of Cross-Validation:\n",
    "\n",
    "*\tHold Out Method: \n",
    "\n",
    "It is the most basic of the CV techniques. It simply divides the dataset into two sets of training and test. The training dataset is used to train the model and then test data is fitted in the trained model to make predictions. We check the accuracy and assess our model on that basis. This method is used as it is computationally less costly. But the evaluation based on the Hold-out set can have a high variance because it depends heavily on which data points end up in the training set and which in test data. The evaluation will be different every time this division changes.\n",
    "\n",
    "*\tk-fold Cross-Validation\n",
    "\n",
    "<img src=\"images/cv1.png\" width=\"\"> \n",
    "\n",
    " img_src:Wikipedia\n",
    "\n",
    "To tackle the high variance of Hold-out method, the k-fold method is used. The idea is simple, divide the whole dataset into ‘k’ sets preferably of equal sizes. Then the first set is selected as the test set and the rest ‘k-1’ sets are used to train the data. Error is calculated for this particular dataset.\n",
    "Then the steps are repeated, i.e. the second set is selected as the test data, and the remaining ‘k-1’ sets are used as the training data. Again, the error is calculated. Similarly, the process continues for ‘k’ times. In the end, the CV error is given as the mean of the total errors calculated individually, mathematically given as:\n",
    "\n",
    "<img src=\"images/cv2.png\" width=\"\"> \n",
    "                                               \n",
    "The variance in error decreases with the increase in ‘k’. The disadvantage of k-fold cv is that it is computationally expensive as the algorithm runs from scratch for ‘k’ times.\n",
    "\n",
    "*  Leave One Out Cross Validation (LOOCV)\n",
    "\n",
    "<img src=\"images/cv3.png\" width=\"\"> \n",
    " \n",
    "LOOCV is a special case of k-fold CV, where k becomes equal to n (number of observations). So instead of creating two subsets, it selects a single observation as a test data and rest of data as the training data. The error is calculated for this test observations. Now, the second observation is selected as test data, and the rest of the data is used as the training set. Again, the error is calculated for this particular test observation. This process continues ‘n’ times and in the end, CV error is calculated as:\n",
    "\n",
    "<img src=\"images/cv4.png\" width=\"\"> \n",
    "                                             \n",
    "\n",
    "### Bias Variance tradeoff for k-fold CV, LOOCV and Holdout Set CV\n",
    "\n",
    "There is a very good explanation given in the ISLR Book as given below:\n",
    "\n",
    "\n",
    "A k-fold CV with k < n has a computational advantage to LOOCV. But putting computational issues aside,\n",
    "a less obvious but potentially more important advantage of k-fold CV is that it often gives more accurate estimates of the test error rate than does LOOCV.\n",
    "The validation set approach can lead to overestimates of the test error rate since in this approach the\n",
    "the training set used to fit the statistical learning method contains only half the observations of the entire data set. Using this logic, it is not hard to see that LOOCV will give approximately unbiased estimates of the test error since each training set contains n − 1 observations, which is almost as many as the number of observations in the full data set. And performing k-fold CV for, say, k = 5 or k = 10 will lead to an intermediate level of bias since each training set contains (k − 1)n/k observations—fewer than\n",
    "in the LOOCV approach, but substantially more than in the validation set approach. Therefore, from the perspective of bias reduction, it is clear that LOOCV is to be preferred to k-fold CV. However, we know that bias is not the only source for concern in an estimating procedure; we must also consider the procedure’s variance. It turns out that LOOCV has higher variance than does k-fold CV with k < n. Why\n",
    "is this the case? When we perform LOOCV, we are in effect averaging the outputs of n fitted models, each of which is trained on an almost identical set of observations; therefore, these outputs are highly (positively) correlated with each other. In contrast, when we perform k-fold CV with k < n, we are averaging the outputs of k fitted models that are somewhat less correlated with each other since the overlap between the training sets in each model is smaller. Since the mean of many highly correlated quantities has higher variance than does the mean of many quantities that are not as highly correlated, the test error estimate resulting from LOOCV tends to have higher variance than does the test error estimate resulting from k-fold CV.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Python implementation of Random Forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mohit Kashyap\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\six.py:31: DeprecationWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
      "  \"(https://pypi.org/project/six/).\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn import tree\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, roc_curve, roc_auc_score\n",
    "from sklearn.externals.six import StringIO  \n",
    "from IPython.display import Image  \n",
    "from sklearn.tree import export_graphviz\n",
    "import pydotplus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.098</td>\n",
       "      <td>25.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.9968</td>\n",
       "      <td>3.20</td>\n",
       "      <td>0.68</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.092</td>\n",
       "      <td>15.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.9970</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.65</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.2</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.075</td>\n",
       "      <td>17.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.9980</td>\n",
       "      <td>3.16</td>\n",
       "      <td>0.58</td>\n",
       "      <td>9.8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0            7.4              0.70         0.00             1.9      0.076   \n",
       "1            7.8              0.88         0.00             2.6      0.098   \n",
       "2            7.8              0.76         0.04             2.3      0.092   \n",
       "3           11.2              0.28         0.56             1.9      0.075   \n",
       "4            7.4              0.70         0.00             1.9      0.076   \n",
       "\n",
       "   free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "0                 11.0                  34.0   0.9978  3.51       0.56   \n",
       "1                 25.0                  67.0   0.9968  3.20       0.68   \n",
       "2                 15.0                  54.0   0.9970  3.26       0.65   \n",
       "3                 17.0                  60.0   0.9980  3.16       0.58   \n",
       "4                 11.0                  34.0   0.9978  3.51       0.56   \n",
       "\n",
       "   alcohol  quality  \n",
       "0      9.4        5  \n",
       "1      9.8        5  \n",
       "2      9.8        5  \n",
       "3      9.8        6  \n",
       "4      9.4        5  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading the dataset\n",
    "import pandas as pd\n",
    "data = pd.read_csv(\"files/winequality_red.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The data set consists following Input variables :\n",
    "\n",
    "1 - fixed acidity  2 - volatile acidity  3 - citric acid  4 - residual sugar  5 - chlorides  6 - free sulfur dioxide\n",
    "7 - total sulfur dioxide  8 - density  9 - pH   10 - sulphates   11 - alcohol\n",
    "\n",
    "and the Output variable gives the quality of th wine based on the input variables: \n",
    "\n",
    "12 - quality (score between 0 and 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1599.000000</td>\n",
       "      <td>1599.000000</td>\n",
       "      <td>1599.000000</td>\n",
       "      <td>1599.000000</td>\n",
       "      <td>1599.000000</td>\n",
       "      <td>1599.000000</td>\n",
       "      <td>1599.000000</td>\n",
       "      <td>1599.000000</td>\n",
       "      <td>1599.000000</td>\n",
       "      <td>1599.000000</td>\n",
       "      <td>1599.000000</td>\n",
       "      <td>1599.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>8.319637</td>\n",
       "      <td>0.527821</td>\n",
       "      <td>0.270976</td>\n",
       "      <td>2.538806</td>\n",
       "      <td>0.087467</td>\n",
       "      <td>15.874922</td>\n",
       "      <td>46.467792</td>\n",
       "      <td>0.996747</td>\n",
       "      <td>3.311113</td>\n",
       "      <td>0.658149</td>\n",
       "      <td>10.422983</td>\n",
       "      <td>5.636023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.741096</td>\n",
       "      <td>0.179060</td>\n",
       "      <td>0.194801</td>\n",
       "      <td>1.409928</td>\n",
       "      <td>0.047065</td>\n",
       "      <td>10.460157</td>\n",
       "      <td>32.895324</td>\n",
       "      <td>0.001887</td>\n",
       "      <td>0.154386</td>\n",
       "      <td>0.169507</td>\n",
       "      <td>1.065668</td>\n",
       "      <td>0.807569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>4.600000</td>\n",
       "      <td>0.120000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.012000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.990070</td>\n",
       "      <td>2.740000</td>\n",
       "      <td>0.330000</td>\n",
       "      <td>8.400000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>7.100000</td>\n",
       "      <td>0.390000</td>\n",
       "      <td>0.090000</td>\n",
       "      <td>1.900000</td>\n",
       "      <td>0.070000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>0.995600</td>\n",
       "      <td>3.210000</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>9.500000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>7.900000</td>\n",
       "      <td>0.520000</td>\n",
       "      <td>0.260000</td>\n",
       "      <td>2.200000</td>\n",
       "      <td>0.079000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>0.996750</td>\n",
       "      <td>3.310000</td>\n",
       "      <td>0.620000</td>\n",
       "      <td>10.200000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>9.200000</td>\n",
       "      <td>0.640000</td>\n",
       "      <td>0.420000</td>\n",
       "      <td>2.600000</td>\n",
       "      <td>0.090000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>0.997835</td>\n",
       "      <td>3.400000</td>\n",
       "      <td>0.730000</td>\n",
       "      <td>11.100000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>15.900000</td>\n",
       "      <td>1.580000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>15.500000</td>\n",
       "      <td>0.611000</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>289.000000</td>\n",
       "      <td>1.003690</td>\n",
       "      <td>4.010000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>14.900000</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       fixed acidity  volatile acidity  citric acid  residual sugar  \\\n",
       "count    1599.000000       1599.000000  1599.000000     1599.000000   \n",
       "mean        8.319637          0.527821     0.270976        2.538806   \n",
       "std         1.741096          0.179060     0.194801        1.409928   \n",
       "min         4.600000          0.120000     0.000000        0.900000   \n",
       "25%         7.100000          0.390000     0.090000        1.900000   \n",
       "50%         7.900000          0.520000     0.260000        2.200000   \n",
       "75%         9.200000          0.640000     0.420000        2.600000   \n",
       "max        15.900000          1.580000     1.000000       15.500000   \n",
       "\n",
       "         chlorides  free sulfur dioxide  total sulfur dioxide      density  \\\n",
       "count  1599.000000          1599.000000           1599.000000  1599.000000   \n",
       "mean      0.087467            15.874922             46.467792     0.996747   \n",
       "std       0.047065            10.460157             32.895324     0.001887   \n",
       "min       0.012000             1.000000              6.000000     0.990070   \n",
       "25%       0.070000             7.000000             22.000000     0.995600   \n",
       "50%       0.079000            14.000000             38.000000     0.996750   \n",
       "75%       0.090000            21.000000             62.000000     0.997835   \n",
       "max       0.611000            72.000000            289.000000     1.003690   \n",
       "\n",
       "                pH    sulphates      alcohol      quality  \n",
       "count  1599.000000  1599.000000  1599.000000  1599.000000  \n",
       "mean      3.311113     0.658149    10.422983     5.636023  \n",
       "std       0.154386     0.169507     1.065668     0.807569  \n",
       "min       2.740000     0.330000     8.400000     3.000000  \n",
       "25%       3.210000     0.550000     9.500000     5.000000  \n",
       "50%       3.310000     0.620000    10.200000     6.000000  \n",
       "75%       3.400000     0.730000    11.100000     6.000000  \n",
       "max       4.010000     2.000000    14.900000     8.000000  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  Getting describe of the data\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Separating Features and labels\n",
    "\n",
    "X = data.drop(columns = 'quality')\n",
    "y = data['quality']\n",
    "\n",
    "#  Splitting Data into Train and Test\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV\n",
    "\n",
    "x_train,x_test,y_train,y_test = train_test_split(X,y,test_size = 0.30, random_state= 355)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision tree classifier score ->  0.6145833333333334\n",
      "Decision tree classifier score ->  0.6270833333333333\n"
     ]
    }
   ],
   "source": [
    "#  Fitting Decision tree Classifiers\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "\n",
    "# Fitting Decision Tre classifiers with different parameters\n",
    "\n",
    "# 1st Tree\n",
    "clf1 = DecisionTreeClassifier( min_samples_split= 2)\n",
    "clf1.fit(x_train,y_train)\n",
    "#2nd Tree\n",
    "clf2 = DecisionTreeClassifier(criterion = 'entropy', max_depth =24, min_samples_leaf= 1)\n",
    "clf2.fit(x_train,y_train)\n",
    "\n",
    "# Accuracy of our classification tree\n",
    "print ('Decision tree classifier score -> ', clf.score(x_test,y_test))\n",
    "print ('Decision tree classifier score -> ', clf2.score(x_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Classifier score ->  0.6708333333333333\n"
     ]
    }
   ],
   "source": [
    "#  Fitting a Random Forest ClASSIFIER\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rand_clf = RandomForestClassifier(random_state=6)     # Give a constant value of random score for a constant accuracy score\n",
    "rand_clf.fit(x_train,y_train)\n",
    "\n",
    "print ('Random Forest Classifier score -> ', rand_clf.score(x_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  We can see that two individual decision trees have both less score than a single Random Forest classifier. \n",
    "\n",
    "#####  So, using Random Forest classifier has increased the predicitive power of our model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        ##############   #############   Parameters for Random Forest   ################   ################\n",
    "    \n",
    " =>  Random forest hyperparameters are a combination of best hyperparameters of both decision tree and Bagging classifier.\n",
    "\n",
    "* Hyperparameters of Decision tree: \n",
    "\n",
    "class_weight=None, criterion='entropy', max_depth=24,max_features=None, max_leaf_nodes=None,\n",
    "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "                       min_samples_leaf=1, min_samples_split=2,\n",
    "                       min_weight_fraction_leaf=0.0, presort=False,\n",
    "                       random_state=None, splitter='best'\n",
    "                    \n",
    "* Hyperparameters of Bagging classifier:\n",
    "\n",
    "base_estimator=None, bootstrap=True, bootstrap_features=False,\n",
    "                  max_features=1.0, max_samples=1.0, n_estimators=10,\n",
    "                  n_jobs=None, oob_score=False, random_state=None, verbose=0,\n",
    "                  warm_start=False\n",
    "                  \n",
    "* Hyperparameters of Random forest classifier:\n",
    "    \n",
    "bootstrap=True, class_weight=None, criterion='gini',\n",
    "                       max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
    "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "                       min_samples_leaf=1, min_samples_split=2,\n",
    "                       min_weight_fraction_leaf=0.0, n_estimators=10,\n",
    "                       n_jobs=None, oob_score=False, random_state=None,\n",
    "                       verbose=0, warm_start=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Lets do some Hyper Parameter tuning for random forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " =>  Lets now try to tune some hyperparameters using the GridSearchCV algorithm.\n",
    "\n",
    " =>  GridSearchCV is a method used to tune our hyperparameters.\n",
    "      We can pass different values of hyperparameters as parameters for grid search.\n",
    "       It does a exhaustive generation of combination of different parameters passed.\n",
    "\n",
    " =>  Using cross validation score, Grid Search returns the combination of hyperparameters for which the model is performing\n",
    "      the best. \n",
    "\n",
    "=> ** Note ->  It is common that a small subset of those parameters can have a large impact on the predictive or computation\n",
    "                  performance of the model while others can be left to their default values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Tuning three hyperparameters right now and passing the different values for both parameters\n",
    "\n",
    "# Creating a Parameter dictionary\n",
    "grid_param = {\n",
    "    \"n_estimators\" : [90,100,115],\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'min_samples_leaf' : [1,2,3,4,5],\n",
    "    'min_samples_split': [4,5,6,7,8],\n",
    "    'max_features' : ['auto','log2']\n",
    "}\n",
    "\n",
    "# Making a GridsearchCV function\n",
    "grid_search = GridSearchCV(estimator=rand_clf,param_grid=grid_param,cv=5,n_jobs =-1,verbose = 3)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#  Tuning the Hyper Parameters of Model\n",
    "grid_search.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'criterion': 'entropy',\n",
       " 'max_features': 'auto',\n",
       " 'min_samples_leaf': 1,\n",
       " 'min_samples_split': 4,\n",
       " 'n_estimators': 115}"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  Getting the Best parameters through GridsearchCV\n",
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final score with best parameters ->  0.6729166666666667\n"
     ]
    }
   ],
   "source": [
    "#  Training again the Model with Best parameters\n",
    "rand_clf = RandomForestClassifier(criterion= 'entropy', max_features = 'auto',  min_samples_leaf = 1,  min_samples_split= 4,\n",
    "                                  n_estimators = 115,random_state=6)\n",
    "rand_clf.fit(x_train,y_train)\n",
    "print ('Final score with best parameters -> ', rand_clf.score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "          ############   #############   Conclusion  #############    ###############\n",
    "\n",
    " =>  Our accuracy has improved and score is better than the last grid search.\n",
    "      So, we can say that giving all the hyperparameters in the gridSearch doesn't gurantee the best result.\n",
    "      We have to do hit and trial with parameters to get the perfect score.\n",
    "\n",
    " =>  You are welcome to try tweaking the parameters more and try an improve the accuracy more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving the Model as a pickle file\n",
    "\n",
    "import pickle\n",
    "with open('Model.sav', 'wb') as f:\n",
    "    pickle.dump(rand_clf,f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cloud  Deployment (Azure)\n",
    "\n",
    "Once the training is completed, we need to expose the trained model as an API for the user to consume it. For prediction, the saved model is loaded first and then the predictions are made using it. If the web app works fine, the same app is deployed to the cloud platform. The application flow for cloud deployment looks like:\n",
    "\n",
    "<img src=\"images/testing_pipeline.PNG\" width= \"300\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-requisites for cloud deployment:\n",
    "* Basic knowledge of flask framework.\n",
    "* Any Python IDE installed(we are using PyCharm).\n",
    "* A Microsoft Azure account.\n",
    "* Basic understanding of HTML.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###\tDeployment to Azure:\n",
    "•\tGo to https://portal.azure.com/ and create an account if already haven’t created one.\n",
    "•\tGo to the Azure account and create a web app.\n",
    "\n",
    "<img src=\"images/dep1.png\">\n",
    " \n",
    "•\tProvide the app name, resource group(create new if necessary), runtime stack(Python 3.7), region, select the 1 GB size, which is free to use. Click Review+create to create the web app.\n",
    " \n",
    "<img src=\"images/dep2.png\">\n",
    "\n",
    "•\tOnce the deployment is completed, open the app and go to the ‘Deployment Center’ option. Select ‘local git’ for source control and click continue.\n",
    " \n",
    "<img src=\"images/dep3.png\">\n",
    "\n",
    "\n",
    "•\tSelect the kudo ‘App service build provider’ as the build provider and click continue.\n",
    "\n",
    "<img src=\"images/dep4.png\">\n",
    " \n",
    "•\tClick ‘Finish’ to complete the setup.\n",
    "•\tGo to the overview section of the app, and the Git link now will be visible.\n",
    "\n",
    "<img src=\"images/dep5.png\">\n",
    " \n",
    "•\tGo to ‘Deployment Credentials’ and copy the username and password. These will be required when doing the final push to the remote git repository.\n",
    "\n",
    "<img src=\"images/dep6.png\">\n",
    " \n",
    "\n",
    "•\tOpen a command prompt and navigate to your project folder.\n",
    "\n",
    "•\tRun git init to initialise an empty git repository\n",
    "\n",
    "•\tCreate a new remote git alias using the command: git remote add <alias> <git clone url>\n",
    "    \n",
    "•\tUse git add . to add all the files to the local git repository.\n",
    "    \n",
    "•\tUse commit –git m “First Commit” to commit the code to the git repo.\n",
    "    \n",
    "•\tPush the code to the remote repo using git push <alias> master –f\n",
    "    \n",
    "•\tThis prompts for a username and password. Provide the same credentials as copied in the step above.\n",
    "    \n",
    "•\tAfter deployment, from the ‘overview’ ¬¬¬section, copy the URL and paste into the browser to see the application running.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
